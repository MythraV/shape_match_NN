import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import os
import csv
import time
import datetime
import subprocess
import argparse

from shape_dataset import ShapeMatchingDatasetSimple, ShapeMatchingDatasetPrecomputed
from network import SiameseUNet
from generate_polygon_dataset import generate_heatmap_target

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 64
LEARNING_RATE = 2e-4
EPOCHS = 50
IMG_SIZE = 128
DATASET_SIZE = 100000
LOSS_SCALE = 100.0

def evaluate(model, loader, device, desc="Eval"):
    model.eval()
    total_loss = 0.0
    steps = 0
    with torch.no_grad():
        for template, query, targets_vector in loader:
            template = template.to(device)
            query = query.to(device)
            targets_vector = targets_vector.to(device)
            
            with autocast(enabled=True):
                raw_heatmaps = generate_heatmap_target(
                    batch_size=template.size(0),
                    img_size=IMG_SIZE,
                    targets=targets_vector[:, 0:2], 
                    device=device,
                    sigma=2.0
                )
                match_flag = targets_vector[:, 2].view(-1, 1, 1, 1)
                final_targets = raw_heatmaps * match_flag
                
                pred_logits = model(template, query)
                pred_heatmap = torch.sigmoid(pred_logits)
                
                diff = (pred_heatmap - final_targets) ** 2
                weights = 1.0 + (50.0 * final_targets)
                loss = (diff * weights).mean() * LOSS_SCALE
                
            total_loss += loss.item()
            steps += 1
            if steps >= 100: break
    return total_loss / steps

def train_unet(train_path, val_path):
    print(f"--- Starting Optimized Training on {DEVICE} ---")
    
    torch.backends.cudnn.benchmark = True
    
    # 1. Dataset Preparation
    if not os.path.exists(train_path):
        print(f"Dataset {train_path} not found. Attempting generation...")
        if train_path == "train_data.pt":
             cmd = ["python", "generate_static_dataset.py", "--size", str(DATASET_SIZE), "--out", train_path]
             subprocess.check_call(cmd)
        else:
             # If custom path is missing, we can't easily gen it there without making assumptions.
             # But for Colab workflow, we expect it to exist or be generated by separate step.
             print(f"Warning: {train_path} missing. Attempting to generate default and move/use it?")
             raise FileNotFoundError(f"Training data not found at {train_path}. Please generate it first.")
    
    print(f"Loading Training Data from {train_path}...")
    train_dataset = ShapeMatchingDatasetPrecomputed(train_path)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=4, 
        pin_memory=True
    )
    
    if not os.path.exists(val_path):
        if val_path == "val_data.pt":
            print("Generating validation set...")
            cmd = ["python", "generate_static_dataset.py", "--size", "5000", "--out", val_path]
            subprocess.check_call(cmd)
        else:
             raise FileNotFoundError(f"Validation data not found at {val_path}")
        
    print(f"Loading Validation Data from {val_path}...")
    val_dataset = ShapeMatchingDatasetPrecomputed(val_path)
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    model = SiameseUNet(n_channels=1).to(DEVICE)
    if os.path.exists("siamese_unet.pth"):
        print("Loading existing checkpoint...")
        try:
            model.load_state_dict(torch.load("siamese_unet.pth", map_location=DEVICE))
        except:
            print("Checkpoint incompatible, starting fresh.")

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)
    scaler = GradScaler() 

    log_file = "training_log.csv"
    if not os.path.exists(log_file):
        with open(log_file, "w", newline="") as f:
            csv.writer(f).writerow(["timestamp", "epoch", "train_loss", "val_loss", "lr"])

    best_loss = float('inf')
    patience = 0
    
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        start_time = time.time()
        
        for batch_idx, (template, query, targets_vector) in enumerate(train_loader):
            template = template.to(DEVICE, non_blocking=True)
            query = query.to(DEVICE, non_blocking=True)
            targets_vector = targets_vector.to(DEVICE, non_blocking=True)

            with autocast(enabled=True):
                raw_heatmaps = generate_heatmap_target(
                    batch_size=template.size(0),
                    img_size=IMG_SIZE,
                    targets=targets_vector[:, 0:2], 
                    device=DEVICE,
                    sigma=2.0
                )
                match_flag = targets_vector[:, 2].view(-1, 1, 1, 1)
                final_targets = raw_heatmaps * match_flag

                pred_logits = model(template, query)
                pred_heatmap = torch.sigmoid(pred_logits)

                diff = (pred_heatmap - final_targets) ** 2
                weights = 1.0 + (50.0 * final_targets)
                loss = (diff * weights).mean() * LOSS_SCALE

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()
            
            if batch_idx % 200 == 0 and batch_idx > 0:
                ts = datetime.datetime.now().strftime('%H:%M:%S')
                print(f"[{ts}] E[{epoch+1}] Step[{batch_idx}] | Loss: {loss.item():.4f}")

        epoch_loss = running_loss / len(train_loader)
        epoch_time = time.time() - start_time
        
        # Eval
        val_loss = evaluate(model, val_loader, DEVICE)
        scheduler.step(val_loss)
        
        ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lr = optimizer.param_groups[0]['lr']
        print(f"[{ts}] Ep {epoch+1} ({epoch_time:.1f}s) | Train: {epoch_loss:.4f} | Val: {val_loss:.4f} | LR: {lr:.2e}")
        
        with open(log_file, "a", newline="") as f:
            csv.writer(f).writerow([ts, epoch+1, epoch_loss, val_loss, lr])

        if val_loss < best_loss:
            best_loss = val_loss
            patience = 0
            torch.save(model.state_dict(), "siamese_unet.pth")
            print(f"Saved Best Model (Val: {best_loss:.4f})")
        else:
            patience += 1
            if patience >= 5:
                print("Early stopping.")
                break

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_data', type=str, default='train_data.pt', help='Path to training .pt file')
    parser.add_argument('--val_data', type=str, default='val_data.pt', help='Path to validation .pt file')
    parser.add_argument('--batch_size', type=int, default=64, help='Batch size for training')
    args = parser.parse_args()
    
    # Update global config or pass it down. 
    # Since BATCH_SIZE is global, we can update it here before calling train_unet, 
    # or pass it as an arg. Passing it is cleaner but requires changing train_unet signature.
    # Given the script structure, updating the global variable is the minimal change.
    BATCH_SIZE = args.batch_size
    
    train_unet(args.train_data, args.val_data)